{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944caf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import json\n",
    "import yaml\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from rich import print as prt\n",
    "from typing import Dict, Any, List\n",
    "\n",
    "sys.path.append(\"../\")\n",
    "from src.utils.config_loader import get_input_path, get_output_path\n",
    "\n",
    "# Configure standard logging for Jupyter\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()],  # This will output to the notebook\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc15ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_source_schema() -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load the source schema from a JSON file.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The loaded JSON schema\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the schema file doesn't exist\n",
    "        json.JSONDecodeError: If the JSON file is malformed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        schema_file = get_input_path(\"schema\")\n",
    "        if not schema_file.exists():\n",
    "            raise FileNotFoundError(f\"Schema file not found: {schema_file}\")\n",
    "\n",
    "        with open(schema_file, \"r\") as f:\n",
    "            schema = json.load(f)\n",
    "\n",
    "        logger.info(f\"Successfully loaded source schema from {schema_file}\")\n",
    "        return schema\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Schema file not found: {e}\")\n",
    "        raise\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Error parsing JSON schema: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d419c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_attribute_mapping() -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load attribute mapping configuration from a YAML file.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, str]]: The attribute mapping dictionary\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the configuration file doesn't exist\n",
    "        yaml.YAMLError: If the YAML file is malformed\n",
    "        KeyError: If required keys are missing from the configuration\n",
    "    \"\"\"\n",
    "    try:\n",
    "        config_file = get_input_path(\"attribute_mapping\")\n",
    "\n",
    "        if not config_file.exists():\n",
    "            raise FileNotFoundError(f\"Configuration file not found: {config_file}\")\n",
    "\n",
    "        with open(config_file, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "\n",
    "        if \"attribute_mapping\" not in config:\n",
    "            raise KeyError(\"'attribute_mapping' key not found in configuration file\")\n",
    "\n",
    "        attribute_mapping = config[\"attribute_mapping\"]\n",
    "\n",
    "        # Validate the structure\n",
    "        for attr_name, mapping in attribute_mapping.items():\n",
    "            if not isinstance(mapping, dict):\n",
    "                raise ValueError(\n",
    "                    f\"Invalid mapping for attribute '{attr_name}': expected dict, got {type(mapping)}\"\n",
    "                )\n",
    "\n",
    "            if \"field\" not in mapping or \"container\" not in mapping:\n",
    "                raise KeyError(\n",
    "                    f\"Missing required keys ('field', 'container') for attribute '{attr_name}'\"\n",
    "                )\n",
    "\n",
    "        logger.info(f\"Successfully loaded attribute mapping from {config_file}\")\n",
    "        return attribute_mapping\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Configuration file not found: {e}\")\n",
    "        raise\n",
    "    except yaml.YAMLError as e:\n",
    "        logger.error(f\"Error parsing YAML configuration: {e}\")\n",
    "        raise\n",
    "    except (KeyError, ValueError) as e:\n",
    "        logger.error(f\"Invalid configuration structure: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e86fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_objects():\n",
    "    \"\"\"\n",
    "    Get the source raw objects from the data folder and store them as a list of dictionaries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        data_path = get_input_path(\"raw_objects\")\n",
    "\n",
    "        with open(data_path, \"r\") as f:\n",
    "            source_objects = json.load(f)\n",
    "\n",
    "        if not isinstance(source_objects, list):\n",
    "            logger.error(\"Source objects should be a list of dictionaries.\")\n",
    "            return []\n",
    "\n",
    "        logger.info(\"Source objects loaded successfully.\")\n",
    "        return source_objects\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"Source objects file not found.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(\"Error decoding JSON from source objects file.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c910eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ism(acm: dict) -> dict:\n",
    "    \"\"\"Extract the reduced 'ism' structure from any ACM dict.\"\"\"\n",
    "    return {\n",
    "        \"banner\": acm.get(\"banner\"),\n",
    "        \"classification\": acm.get(\"classif\"),\n",
    "        \"ownerProducer\": acm.get(\"owner_prod\"),\n",
    "        \"releaseableTo\": acm.get(\"rel_to\"),\n",
    "        'disseminationControls': acm.get(\"dissem_ctrls\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d9190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_object(target_structure: Dict[str, Any], attr_index: Dict[str, Dict], attribute_map: Dict[str, Dict[str, str]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build a standard object by mapping attributes from the source data to target fields.\n",
    "    \n",
    "    This function takes a pre-initialized target_structure dictionary and populates it with\n",
    "    transformed attribute values based on the provided attribute mapping. Each\n",
    "    attribute value is wrapped with ISM classification metadata.\n",
    "    \n",
    "    Args:\n",
    "        target_structure (Dict[str, Any]): Pre-initialized dictionary containing basic object metadata\n",
    "            and empty containers (ontology, maritimeMetadata, equipment, facility)\n",
    "        \n",
    "        attr_index (Dict[str, Dict]): Index of attribute data items keyed by attribute name,\n",
    "            where each item contains 'attributeValue' and 'acm' fields\n",
    "        \n",
    "        attribute_map (Dict[str, Dict[str, str]]): Mapping configuration where keys are attribute names and values are dicts with\n",
    "            'field' and 'container' specifications\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: The populated target_structure dictionary with mapped attributes organized\n",
    "            into their designated containers, or empty dict if an error occurs\n",
    "    \n",
    "    Note:\n",
    "        - Attributes mapped to \"root\" container are placed directly in the target_structure dict\n",
    "        - Other containers are nested under their respective keys\n",
    "        - Each mapped value includes the original value and ISM classification metadata\n",
    "        - Missing attributes in attr_index are silently skipped\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for attr_name, mapping in attribute_map.items():\n",
    "            item = attr_index.get(attr_name)\n",
    "            \n",
    "            if not item:\n",
    "                continue\n",
    "            \n",
    "            target_field = mapping[\"field\"]\n",
    "            container = mapping[\"container\"]\n",
    "            \n",
    "            transformed_value = {\n",
    "                \"value\": item.get(\"attributeValue\"),\n",
    "                \"ism\": extract_ism(item.get(\"acm\", {}))\n",
    "            }\n",
    "            \n",
    "            if container == \"root\":\n",
    "                target_structure[target_field] = transformed_value\n",
    "            else:\n",
    "                # Ensure nested container exists\n",
    "                if container not in target_structure:\n",
    "                    target_structure[container] = {}\n",
    "                target_structure[container][target_field] = transformed_value\n",
    "        \n",
    "        return target_structure\n",
    "    except Exception as e:\n",
    "        print(f\"Error building standard object: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8628fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_source_object(source: Dict[str, Any], attribute_map: Dict[str, Dict[str, str]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Transform a source object into a structured format based on the provided attribute mapping.\n",
    "    \n",
    "    Args:\n",
    "        source: The source dictionary containing object data with attributes, ACM, and metadata\n",
    "        attribute_map: Dictionary mapping attribute names to their target field and container locations\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing the transformed object with structured fields including:\n",
    "        - Basic metadata (version, id, name, etc.)\n",
    "        - Overall classification from ACM\n",
    "        - Mapped attributes organized into appropriate containers (root, ontology, maritimeMetadata, facility)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if source is a valid dictionary\n",
    "        if not isinstance(source, dict):\n",
    "            logger.error(\"Source object is not a valid dictionary.\")\n",
    "            return {}\n",
    "        \n",
    "        # Check that attribute_map is provided\n",
    "        if not attribute_map or not isinstance(attribute_map, dict):\n",
    "            logger.error(\"Attribute map is not provided or is not a valid dictionary.\")\n",
    "            return {}\n",
    "        \n",
    "        target_structure = {\n",
    "            \"version\": source.get(\"version\"),\n",
    "            \"overallClassification\": extract_ism(source.get(\"acm\", {})),\n",
    "            \"id\": source.get(\"id\"),\n",
    "            \"name\": source.get(\"name\"),\n",
    "            \"lastUpdatedDate\": source.get(\"lastVerified\", {}).get(\"timestamp\"),\n",
    "            \"excerciseIndicator\": source.get(\"gide_id\"),\n",
    "        }\n",
    "        \n",
    "        # Pre-initialize containers\n",
    "        target_structure[\"maritimeMetadata\"] = {}\n",
    "        target_structure[\"ontology\"] = {}\n",
    "        target_structure[\"equipment\"] = {}\n",
    "        target_structure[\"facility\"] = {}\n",
    "        \n",
    "        # Build quick index for attributes.data\n",
    "        data_items = source.get(\"attributes\", {}).get(\"data\", [])\n",
    "        attr_index = {item.get(\"attributeName\"): item for item in data_items}\n",
    "        \n",
    "        standard_object = build_standard_object(target_structure, attr_index, attribute_map)\n",
    "        \n",
    "        print(f\"Finished transforming object with ID: {standard_object.get('id', 'unknown')}\")\n",
    "        return standard_object\n",
    "    except Exception as e:\n",
    "        print(f\"Error transforming object with ID {source.get('id', 'unknown')}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b737c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_containers(obj: Dict[str, Any], container_keys: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Remove empty containers from a standard object.\n",
    "    \n",
    "    This function removes any container dictionaries that are empty, helping to \n",
    "    clean up the object structure and reduce noise in the final output.\n",
    "    \n",
    "    Args:\n",
    "        obj (Dict[str, Any]): The standard object with potentially empty containers\n",
    "        container_keys (List[str]): List of keys representing containers to check for emptiness\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: The cleaned object with empty containers removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the object to avoid modifying the original\n",
    "    cleaned_obj = obj.copy()\n",
    "    \n",
    "    # Remove empty containers\n",
    "    for container_key in container_keys:\n",
    "        if container_key in cleaned_obj:\n",
    "            container = cleaned_obj[container_key]\n",
    "            # Remove if container is empty dict or None\n",
    "            if not container or (isinstance(container, dict) and len(container) == 0):\n",
    "                del cleaned_obj[container_key]\n",
    "    \n",
    "    return cleaned_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1fe44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_standard_objects(cleaned_objects: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"\n",
    "    Save each cleaned standard object to a separate JSON file.\n",
    "    \n",
    "    This function saves each cleaned standard object to a JSON file with a filename\n",
    "    based on the object ID and current timestamp. It handles file writing errors\n",
    "    and ensures proper JSON formatting.\n",
    "    \n",
    "    Args:\n",
    "        cleaned_objects (List[Dict[str, Any]]): List of cleaned standard objects to save\n",
    "    \n",
    "    Returns:\n",
    "        None: The function does not return a value, but logs the results of the save operation.\n",
    "    \n",
    "    Raises:\n",
    "        OSError: If the output directory cannot be created or accessed\n",
    "    \"\"\"\n",
    "    output_path = get_output_path(\"processed_dir\")\n",
    "    \n",
    "    # Generate timestamp for this batch\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    for i, obj in enumerate(cleaned_objects):\n",
    "        try:\n",
    "            # Get object ID, fallback to index if ID is missing\n",
    "            obj_id = obj.get(\"id\", f\"object_{i}\")\n",
    "            \n",
    "            # Create filename with object ID and timestamp\n",
    "            filename = f\"{obj_id}_{timestamp}.json\"\n",
    "            file_path = output_path / filename\n",
    "            \n",
    "            # Ensure the object is JSON serializable\n",
    "            if not isinstance(obj, dict):\n",
    "                raise ValueError(f\"Object {i} is not a valid dictionary\")\n",
    "            \n",
    "            # Write JSON file with proper formatting\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(obj, f, indent=2, ensure_ascii=False, sort_keys=True)\n",
    "            \n",
    "            logger.info(f\"Successfully saved object {obj_id} to {filename}\")\n",
    "        except (ValueError, TypeError) as e:\n",
    "            error_msg = f\"Object {i} serialization error: {e}\"\n",
    "            logger.error(error_msg)\n",
    "            raise\n",
    "        except OSError as e:\n",
    "            error_msg = f\"File write error for object {i}: {e}\"\n",
    "            logger.error(error_msg)\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Unexpected error saving object {i}: {e}\"\n",
    "            logger.error(error_msg)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee51c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the source objects\n",
    "source_objects = get_source_objects()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load attribute mapping from configuration file\n",
    "attribute_mapping = load_attribute_mapping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d133fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_objects = [transform_source_object(obj, attribute_mapping) for obj in source_objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99744eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the container keys that should be checked for emptiness\n",
    "container_keys = [\"maritimeMetadata\", \"ontology\", \"equipment\", \"facility\"]\n",
    "\n",
    "# Apply the cleanup function to all standard objects\n",
    "cleaned_standard_objects = [remove_empty_containers(obj, container_keys) for obj in standard_objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned standard objects to JSON files\n",
    "save_standard_objects(cleaned_standard_objects)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
