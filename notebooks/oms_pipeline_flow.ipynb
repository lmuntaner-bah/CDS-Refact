{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "944caf65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import yaml\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from rich import print as prt\n",
    "from typing import Dict, Any, List, Optional\n",
    "\n",
    "# Configure standard logging for Jupyter\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s - %(name)s - %(levelname)s - %(message)s\",\n",
    "    handlers=[logging.StreamHandler()],  # This will output to the notebook\n",
    ")\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d419c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_attribute_mapping(config_file: str) -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load attribute mapping configuration from a YAML file.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, str]]: The attribute mapping dictionary\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the configuration file doesn't exist\n",
    "        yaml.YAMLError: If the YAML file is malformed\n",
    "        KeyError: If required keys are missing from the configuration\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not config_file:\n",
    "            raise FileNotFoundError(f\"Configuration file not found: {config_file}\")\n",
    "\n",
    "        with open(config_file, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "\n",
    "        if \"attribute_mapping\" not in config:\n",
    "            raise KeyError(\"'attribute_mapping' key not found in configuration file\")\n",
    "\n",
    "        attribute_mapping = config[\"attribute_mapping\"]\n",
    "\n",
    "        # Validate the structure\n",
    "        for attr_name, mapping in attribute_mapping.items():\n",
    "            if not isinstance(mapping, dict):\n",
    "                raise ValueError(\n",
    "                    f\"Invalid mapping for attribute '{attr_name}': expected dict, got {type(mapping)}\"\n",
    "                )\n",
    "\n",
    "            if \"field\" not in mapping or \"container\" not in mapping:\n",
    "                raise KeyError(\n",
    "                    f\"Missing required keys ('field', 'container') for attribute '{attr_name}'\"\n",
    "                )\n",
    "\n",
    "        logger.info(f\"Successfully loaded attribute mapping from {config_file}\")\n",
    "        return attribute_mapping\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Configuration file not found: {e}\")\n",
    "        raise\n",
    "    except yaml.YAMLError as e:\n",
    "        logger.error(f\"Error parsing YAML configuration: {e}\")\n",
    "        raise\n",
    "    except (KeyError, ValueError) as e:\n",
    "        logger.error(f\"Invalid configuration structure: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d3e86fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_objects(data_path: str):\n",
    "    \"\"\"\n",
    "    Get the source raw objects from the data folder and store them as a list of dictionaries.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(data_path, \"r\") as f:\n",
    "            source_objects = json.load(f)\n",
    "\n",
    "        if not isinstance(source_objects, list):\n",
    "            logger.error(\"Source objects should be a list of dictionaries.\")\n",
    "            return []\n",
    "\n",
    "        logger.info(\"Source objects loaded successfully.\")\n",
    "        return source_objects\n",
    "    except FileNotFoundError:\n",
    "        logger.error(\"Source objects file not found.\")\n",
    "        return []\n",
    "    except json.JSONDecodeError:\n",
    "        logger.error(\"Error decoding JSON from source objects file.\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c910eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_ism(acm: dict) -> dict:\n",
    "    \"\"\"Extract the reduced 'ism' structure from any ACM dict.\"\"\"\n",
    "    return {\n",
    "        \"banner\": acm.get(\"banner\"),\n",
    "        \"classification\": acm.get(\"classif\"),\n",
    "        \"ownerProducer\": acm.get(\"owner_prod\"),\n",
    "        \"releaseableTo\": acm.get(\"rel_to\"),\n",
    "        'disseminationControls': acm.get(\"dissem_ctrls\"),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fcf6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_elevation(source_object: Dict[str, Any]) -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Retrieves the elevation value from the source object, handling variations\n",
    "    in the attribute name (e.g., \"Elevation\", \"Elevation(m)\", \"Elevation (m)\").\n",
    "\n",
    "    Args:\n",
    "        source_object (Dict[str, Any]): The source JSON-like object.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Any]: The elevation value if found, otherwise None.\n",
    "    \"\"\"\n",
    "    elevation_value = None\n",
    "    \n",
    "    # Define possible variations of the \"Elevation\" attribute name\n",
    "    elevation_variations = [\"elevation\", \"elevation(m)\", \"elevation (m)\"]\n",
    "    try:\n",
    "        # Ensure the source object is a dictionary and contains the expected structure\n",
    "        if not isinstance(source_object, dict):\n",
    "            raise ValueError(\"source object must be a dictionary.\")\n",
    "        \n",
    "        if \"attributes\" not in source_object or \"data\" not in source_object[\"attributes\"]:\n",
    "            raise KeyError(\"source object does not contain the expected 'attributes.data' structure.\")\n",
    "        \n",
    "        # Iterate through the attributes to find the elevation value\n",
    "        for attr in source_object[\"attributes\"][\"data\"]:\n",
    "            attribute_name = attr.get(\"attributeName\", \"\").lower()\n",
    "            \n",
    "            if attribute_name in elevation_variations and attr.get(\"attributeValue\") is not None:\n",
    "                elevation_value = attr.get(\"attributeValue\")\n",
    "                break  # Exit the loop once the elevation value is found\n",
    "    except Exception as e:\n",
    "        # Log the exception for debugging purposes\n",
    "        print(f\"Error occurred while retrieving elevation: {e}\")\n",
    "    \n",
    "    return elevation_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d9190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_object(target_structure: Dict[str, Any], attr_index: Dict[str, Dict], attribute_map: Dict[str, Dict[str, str]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build a standard object by mapping attributes from the source data to target fields.\n",
    "    \n",
    "    This function takes a pre-initialized target_structure dictionary and populates it with\n",
    "    transformed attribute values based on the provided attribute mapping. Each\n",
    "    attribute value is wrapped with ISM classification metadata.\n",
    "    \n",
    "    Args:\n",
    "        target_structure (Dict[str, Any]): Pre-initialized dictionary containing basic object metadata\n",
    "            and empty containers (ontology, maritimeMetadata, equipment, facility)\n",
    "        \n",
    "        attr_index (Dict[str, Dict]): Index of attribute data items keyed by attribute name,\n",
    "            where each item contains 'attributeValue' and 'acm' fields\n",
    "        \n",
    "        attribute_map (Dict[str, Dict[str, str]]): Mapping configuration where keys are attribute names and values are dicts with\n",
    "            'field' and 'container' specifications\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: The populated target_structure dictionary with mapped attributes organized\n",
    "            into their designated containers, or empty dict if an error occurs\n",
    "    \n",
    "    Note:\n",
    "        - Attributes mapped to \"root\" container are placed directly in the target_structure dict\n",
    "        - Other containers are nested under their respective keys\n",
    "        - Each mapped value includes the original value and ISM classification metadata\n",
    "        - Missing attributes in attr_index are silently skipped\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for attr_name, mapping in attribute_map.items():\n",
    "            item = attr_index.get(attr_name)\n",
    "            \n",
    "            if not item:\n",
    "                continue\n",
    "            \n",
    "            target_field = mapping[\"field\"]\n",
    "            container = mapping[\"container\"]\n",
    "            \n",
    "            transformed_value = {\n",
    "                \"value\": item.get(\"attributeValue\"),\n",
    "                \"ism\": extract_ism(item.get(\"acm\", {}))\n",
    "            }\n",
    "            \n",
    "            if container == \"root\":\n",
    "                target_structure[target_field] = transformed_value\n",
    "            else:\n",
    "                # Ensure nested container exists\n",
    "                if container not in target_structure:\n",
    "                    target_structure[container] = {}\n",
    "                target_structure[container][target_field] = transformed_value\n",
    "        \n",
    "        return target_structure\n",
    "    except Exception as e:\n",
    "        print(f\"Error building standard object: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_attribute_index(source: Dict[str, Any]) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Prepare a complete attribute index including both standard attributes and top-level fields.\n",
    "    \n",
    "    Args:\n",
    "        source: Source dictionary containing object data\n",
    "        \n",
    "    Returns:\n",
    "        Dict[str, Dict]: Attribute index with both regular attributes and transformed top-level fields\n",
    "    \"\"\"\n",
    "    # Get standard attributes from data items\n",
    "    data_items = source.get(\"attributes\", {}).get(\"data\", [])\n",
    "    attr_index = {item.get(\"attributeName\"): item for item in data_items}\n",
    "    \n",
    "    # Define top-level fields to be included in attribute mapping\n",
    "    top_level_fields = {\n",
    "        \"domain\": \"domain\",\n",
    "        \"allegience\": \"allegience\",\n",
    "        \"allegienceAor\": \"allegienceAor\"\n",
    "        # Add other top-level fields here as needed\n",
    "    }\n",
    "    \n",
    "    # Add top-level fields to attribute index with proper structure\n",
    "    for source_field, attr_name in top_level_fields.items():\n",
    "        if source_field in source:\n",
    "            attr_index[attr_name] = {\n",
    "                \"attributeValue\": source[source_field],\n",
    "                \"acm\": source.get(\"acm\", {})\n",
    "            }\n",
    "    \n",
    "    return attr_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7259309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_location(source_object: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Processes location information from the input object's geographic data.\n",
    "\n",
    "    Args:\n",
    "        source_object (Dict[str, Any]): The input object containing location data.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Processed location data, or None if data is invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        location_data = source_object.get(\"latestKnownLocation\")\n",
    "        if not location_data:\n",
    "            logger.debug(f\"No location data found for object {source_object.get('id')}\")\n",
    "            return None\n",
    "\n",
    "        geometry_data = location_data.get(\"geometry\")\n",
    "        if not geometry_data:\n",
    "            logger.debug(f\"No geometry data found for object {source_object.get('id')}\")\n",
    "            return None\n",
    "\n",
    "        coords = geometry_data.get(\"coordinates\")\n",
    "        if not coords or len(coords) != 2:\n",
    "            logger.debug(f\"Invalid coordinates for object {source_object.get('id')}\")\n",
    "            return None\n",
    "\n",
    "        elevation_value = extract_elevation(source_object)\n",
    "        if isinstance(elevation_value, str):\n",
    "            try:\n",
    "                elevation_value = float(elevation_value)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Error transforming elevation into float: {e}\")\n",
    "                elevation_value = None\n",
    "\n",
    "        return {\n",
    "            \"ism\": extract_ism(location_data.get(\"acm\", {})),\n",
    "            \"id\": location_data.get(\"id\"),\n",
    "            \"timestamp\": location_data.get(\"lastVerified\", {}).get(\"timestamp\"),\n",
    "            \"latitude\": coords[1],\n",
    "            \"longitude\": coords[0],\n",
    "            \"semiMajorError\": None,\n",
    "            \"semiMinorError\": None,\n",
    "            \"errorOrientation\": None,\n",
    "            \"altitude\": {\n",
    "                \"value\": None,\n",
    "                \"quality\": None,\n",
    "                \"error\": None,\n",
    "                \"units\": {\"value\": None},\n",
    "            },\n",
    "            \"elevation\": {\n",
    "                \"value\": elevation_value,\n",
    "                \"quality\": None,\n",
    "                \"error\": None,\n",
    "                \"units\": {\"value\": None},\n",
    "            },\n",
    "            \"derivation\": geometry_data.get(\"type\"),\n",
    "            \"quality\": None,\n",
    "            \"locationName\": None,\n",
    "            \"reason\": None,\n",
    "            \"custom\": None,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Unexpected error writing location values for object {source_object.get('id')}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de008db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Create the functions that handle extracting the Ship Name, Ship Class, and Facility Name.\n",
    "\n",
    "# TODO: Create functions for handling invalid classifications, sci, controls, groups, and terms (e.g. TOP SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dd2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_source_object(source: Dict[str, Any], attribute_map: Dict[str, Dict[str, str]]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Transform a source object into a structured format based on the provided attribute mapping.\n",
    "    \n",
    "    Args:\n",
    "        source: The source dictionary containing object data with attributes, ACM, and metadata\n",
    "        attribute_map: Dictionary mapping attribute names to their target field and container locations\n",
    "    \n",
    "    Returns:\n",
    "        Dict containing the transformed object with structured fields including:\n",
    "        - Basic metadata (version, id, name, etc.)\n",
    "        - Overall classification from ACM\n",
    "        - Mapped attributes organized into appropriate containers (root, ontology, maritimeMetadata, facility)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(source, dict) or not isinstance(attribute_map, dict):\n",
    "            logger.error(\"Invalid source object or attribute map.\")\n",
    "            return {}\n",
    "        \n",
    "        # Special handling for createdDate. This is done because \"Date Of Introduction\" is an attribute nested within data \n",
    "        try:\n",
    "            created_date = None\n",
    "            \n",
    "            for attr in source[\"attributes\"][\"data\"]:\n",
    "                if attr.get(\"attributeName\") in [\"Date Of Introduction\"]:\n",
    "                    created_date = attr.get(\"attributeValue\")\n",
    "        except Exception as e:\n",
    "            print(f\"There was an error setting createdDate: {e}\")\n",
    "        \n",
    "        # Initialize target structure with basic metadata\n",
    "        target_structure = {\n",
    "            \"version\": source.get(\"version\"),\n",
    "            \"overallClassification\": extract_ism(source.get(\"acm\", {})),\n",
    "            \"id\": source.get(\"id\"),\n",
    "            \"name\": source.get(\"name\"),\n",
    "            \"createdDate\": created_date,  # Use the extracted created date\n",
    "            \"lastUpdatedDate\": source.get(\"lastVerified\", {}).get(\"timestamp\"),\n",
    "            \"excerciseIndicator\": source.get(\"gide_id\"),\n",
    "            \"location\": parse_location(source),\n",
    "            # Initialize containers\n",
    "            \"maritimeMetadata\": {},\n",
    "            \"ontology\": {},\n",
    "            \"equipment\": {},\n",
    "            \"facility\": {}\n",
    "        }\n",
    "        \n",
    "        # Get complete attribute index including top-level fields\n",
    "        attr_index = prepare_attribute_index(source)\n",
    "        \n",
    "        # Build and return the standard object\n",
    "        standard_object = build_standard_object(target_structure, attr_index, attribute_map)\n",
    "        \n",
    "        logger.info(f\"Finished transforming object with ID: {standard_object.get('id', 'unknown')}\")\n",
    "        return standard_object\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error transforming object with ID {source.get('id', 'unknown')}: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b737c09b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_containers(obj: Dict[str, Any], container_keys: List[str]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Remove empty containers from a standard object.\n",
    "    \n",
    "    This function removes any container dictionaries that are empty, helping to \n",
    "    clean up the object structure and reduce noise in the final output.\n",
    "    \n",
    "    Args:\n",
    "        obj (Dict[str, Any]): The standard object with potentially empty containers\n",
    "        container_keys (List[str]): List of keys representing containers to check for emptiness\n",
    "    \n",
    "    Returns:\n",
    "        Dict[str, Any]: The cleaned object with empty containers removed\n",
    "    \"\"\"\n",
    "    \n",
    "    # Create a copy of the object to avoid modifying the original\n",
    "    cleaned_obj = obj.copy()\n",
    "    \n",
    "    # Remove empty containers\n",
    "    for container_key in container_keys:\n",
    "        if container_key in cleaned_obj:\n",
    "            container = cleaned_obj[container_key]\n",
    "            # Remove if container is empty dict or None\n",
    "            if not container or (isinstance(container, dict) and len(container) == 0):\n",
    "                del cleaned_obj[container_key]\n",
    "    \n",
    "    return cleaned_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1fe44d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_standard_objects(output_path: str, cleaned_objects: List[Dict[str, Any]]) -> None:\n",
    "    \"\"\"\n",
    "    Save each cleaned standard object to a separate JSON file.\n",
    "    \n",
    "    This function saves each cleaned standard object to a JSON file with a filename\n",
    "    based on the object ID and current timestamp. It handles file writing errors\n",
    "    and ensures proper JSON formatting.\n",
    "    \n",
    "    Args:\n",
    "        cleaned_objects (List[Dict[str, Any]]): List of cleaned standard objects to save\n",
    "    \n",
    "    Returns:\n",
    "        None: The function does not return a value, but logs the results of the save operation.\n",
    "    \n",
    "    Raises:\n",
    "        OSError: If the output directory cannot be created or accessed\n",
    "    \"\"\"\n",
    "    # Generate timestamp for this batch\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "    \n",
    "    for i, obj in enumerate(cleaned_objects):\n",
    "        try:\n",
    "            # Get object ID, fallback to index if ID is missing\n",
    "            obj_id = obj.get(\"id\", f\"object_{i}\")\n",
    "            \n",
    "            # Create filename with object ID and timestamp\n",
    "            filename = f\"{obj_id}_{timestamp}.json\"\n",
    "            file_path = output_path / filename\n",
    "            \n",
    "            # Ensure the object is JSON serializable\n",
    "            if not isinstance(obj, dict):\n",
    "                raise ValueError(f\"Object {i} is not a valid dictionary\")\n",
    "            \n",
    "            # Write JSON file with proper formatting\n",
    "            with open(file_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(obj, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            logger.info(f\"Successfully saved object {obj_id} to {filename}\")\n",
    "        except (ValueError, TypeError) as e:\n",
    "            error_msg = f\"Object {i} serialization error: {e}\"\n",
    "            logger.error(error_msg)\n",
    "            raise\n",
    "        except OSError as e:\n",
    "            error_msg = f\"File write error for object {i}: {e}\"\n",
    "            logger.error(error_msg)\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            error_msg = f\"Unexpected error saving object {i}: {e}\"\n",
    "            logger.error(error_msg)\n",
    "            raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee51c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the source objects\n",
    "data_path = \"../data/1_raw/source_objects.json\"\n",
    "source_objects = get_source_objects(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1096800a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load attribute mapping from configuration file\n",
    "config_file = \"../config/attribute_mapping.yaml\"\n",
    "attribute_mapping = load_attribute_mapping(config_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86d133fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "standard_objects = [transform_source_object(obj, attribute_mapping) for obj in source_objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "284c0c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "prt(standard_objects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99744eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the container keys that should be checked for emptiness\n",
    "container_keys = [\"maritimeMetadata\", \"ontology\", \"equipment\", \"facility\"]\n",
    "\n",
    "# Apply the cleanup function to all standard objects\n",
    "cleaned_standard_objects = [remove_empty_containers(obj, container_keys) for obj in standard_objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70516688",
   "metadata": {},
   "outputs": [],
   "source": [
    "prt(cleaned_standard_objects[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9ce4adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned standard objects to JSON files\n",
    "# output_path = \"../data/2_processed/\"\n",
    "# save_standard_objects(output_path, cleaned_standard_objects)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
