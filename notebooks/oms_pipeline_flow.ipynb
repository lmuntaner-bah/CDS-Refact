{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "944caf65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import yaml\n",
    "from loguru import logger\n",
    "from databricks import sql\n",
    "from datetime import datetime\n",
    "from rich import print as prt\n",
    "from dotenv import load_dotenv\n",
    "from difflib import get_close_matches\n",
    "from typing import Dict, Any, List, Optional\n",
    "from jsonschema import validate, ValidationError\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa96af2e",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef6bb95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_all_objects(directory: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Load all JSON files from a specified directory into a list of dictionaries.\n",
    "\n",
    "    Args:\n",
    "        directory (str): Path to the directory containing JSON files.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]: List of dictionaries, each representing the contents of a JSON file.\n",
    "\n",
    "    Note:\n",
    "        This function mimics querying data from an API by reading local files.\n",
    "        In production, replace this logic with actual API calls as needed.\n",
    "    \"\"\"\n",
    "    json_list = []\n",
    "\n",
    "    for current_file in os.scandir(directory):\n",
    "        if current_file.is_file() and current_file.name.endswith(\".json\"):\n",
    "            with open(current_file.path, \"r\") as json_file:\n",
    "                json_data = json.load(json_file)\n",
    "                json_list.append(json_data)\n",
    "\n",
    "    logger.info(\"Source objects loaded successfully.\")\n",
    "    return json_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d419c6fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_attribute_mapping(config_file: str) -> Dict[str, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Load attribute mapping configuration from a YAML file.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict[str, str]]: The attribute mapping dictionary\n",
    "\n",
    "    Raises:\n",
    "        FileNotFoundError: If the configuration file doesn't exist\n",
    "        yaml.YAMLError: If the YAML file is malformed\n",
    "        KeyError: If required keys are missing from the configuration\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not config_file:\n",
    "            raise FileNotFoundError(f\"Configuration file not found: {config_file}\")\n",
    "\n",
    "        with open(config_file, \"r\") as f:\n",
    "            config = yaml.safe_load(f)\n",
    "\n",
    "        if \"attribute_mapping\" not in config:\n",
    "            raise KeyError(\"'attribute_mapping' key not found in configuration file\")\n",
    "\n",
    "        attribute_mapping = config[\"attribute_mapping\"]\n",
    "\n",
    "        # Validate the structure\n",
    "        for attr_name, mapping in attribute_mapping.items():\n",
    "            if not isinstance(mapping, dict):\n",
    "                raise ValueError(\n",
    "                    f\"Invalid mapping for attribute '{attr_name}': expected dict, got {type(mapping)}\"\n",
    "                )\n",
    "\n",
    "            if \"field\" not in mapping or \"container\" not in mapping:\n",
    "                raise KeyError(\n",
    "                    f\"Missing required keys ('field', 'container') for attribute '{attr_name}'\"\n",
    "                )\n",
    "\n",
    "        logger.info(f\"Successfully loaded attribute mapping from {config_file}\")\n",
    "        return attribute_mapping\n",
    "\n",
    "    except FileNotFoundError as e:\n",
    "        logger.error(f\"Configuration file not found: {e}\")\n",
    "        raise\n",
    "    except yaml.YAMLError as e:\n",
    "        logger.error(f\"Error parsing YAML configuration: {e}\")\n",
    "        raise\n",
    "    except (KeyError, ValueError) as e:\n",
    "        logger.error(f\"Invalid configuration structure: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "121f7fef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_valid_attribute_names(file_path: str) -> list:\n",
    "    \"\"\"\n",
    "    Loads the list of valid attribute names from a YAML file.\n",
    "\n",
    "    Args:\n",
    "    - file_path (str): Path to the YAML file containing valid attribute names.\n",
    "\n",
    "    Returns:\n",
    "    - list: A list of valid attribute names.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(file_path, \"r\") as file:\n",
    "            data = yaml.safe_load(file)\n",
    "            logger.info(\"Loaded valid attribute names successfully.\")\n",
    "            return data.get(\"valid_attribute_names\", [])\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error loading valid attribute names: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3aa35c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_classification_config(config_path: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load classification configuration from a JSON file.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(config_path, \"r\") as file:\n",
    "            config = yaml.safe_load(file)\n",
    "            if \"restrictions\" not in config:\n",
    "                raise ValueError(\"Missing 'restrictions' key in classification config\")\n",
    "\n",
    "            logger.info(\"Classification configuration loaded successfully.\")\n",
    "            return config[\"restrictions\"]\n",
    "    except FileNotFoundError:\n",
    "        raise FileNotFoundError(f\"Configuration file not found: {config_path}\")\n",
    "    except yaml.YAMLError as e:\n",
    "        raise ValueError(f\"Error parsing YAML file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b800425d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_standard_object_schema(schema_path: str) -> dict:\n",
    "    \"\"\"\n",
    "    Load the JSON schema from a file.\n",
    "\n",
    "    Args:\n",
    "        schema_path (str): Path to the JSON schema\n",
    "    Returns:\n",
    "        dict: The loaded JSON schema\n",
    "    \"\"\"\n",
    "    try:\n",
    "        with open(schema_path, \"r\") as schema_file:\n",
    "            schema = json.load(schema_file)\n",
    "        logger.info(f\"Schema loaded successfully from {schema_path}\")\n",
    "        return schema\n",
    "    except FileNotFoundError:\n",
    "        logger.error(f\"Schema file not found at {schema_path}\")\n",
    "        return {}\n",
    "    except json.JSONDecodeError as e:\n",
    "        logger.error(f\"Invalid JSON in schema file: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3640fd1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_standard_objects(\n",
    "    output_path: str, cleaned_objects: List[Dict[str, Any]]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Save each cleaned standard object to a separate JSON file.\n",
    "\n",
    "    This function saves each cleaned standard object to a JSON file with a filename\n",
    "    based on the object ID and current timestamp. It handles file writing errors\n",
    "    and ensures proper JSON formatting.\n",
    "\n",
    "    Args:\n",
    "        cleaned_objects (List[Dict[str, Any]]): List of cleaned standard objects to save\n",
    "\n",
    "    Returns:\n",
    "        None: The function does not return a value, but logs the results of the save operation.\n",
    "\n",
    "    Raises:\n",
    "        OSError: If the output directory cannot be created or accessed\n",
    "    \"\"\"\n",
    "    # Generate timestamp for this batch\n",
    "    timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "\n",
    "    for i, obj in enumerate(cleaned_objects):\n",
    "        try:\n",
    "            # Get object ID, fallback to index if ID is missing\n",
    "            obj_id = obj.get(\"id\", f\"object_{i}\")\n",
    "\n",
    "            # Create filename with object ID and timestamp\n",
    "            filename = f\"{obj_id}_{timestamp}.json\"\n",
    "            file_path = os.path.join(output_path, filename)\n",
    "\n",
    "            # Ensure the object is JSON serializable\n",
    "            if not isinstance(obj, dict):\n",
    "                raise ValueError(f\"Object {i} is not a valid dictionary\")\n",
    "\n",
    "            # Write JSON file with proper formatting\n",
    "            with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "                json.dump(obj, f, indent=2)\n",
    "\n",
    "            logger.info(f\"Successfully saved object {obj_id} to {filename}\")\n",
    "        except (ValueError, TypeError) as e:\n",
    "            logger.error(f\"Object {i} serialization error: {e}\")\n",
    "            raise\n",
    "        except OSError as e:\n",
    "            logger.error(f\"File write error for object {i}: {e}\")\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error saving object {i}: {e}\")\n",
    "            raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d7e59f",
   "metadata": {},
   "source": [
    "### Attribute Name Drift Detector Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f4623f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capture_unexpected_attributes(\n",
    "    source_objects: List[Dict[str, Any]], valid_attribute_names: List[str]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Captures unexpected attribute names from source objects.\n",
    "\n",
    "    Args:\n",
    "    - source_objects (List[Dict[str, Any]]): A list of JSON objects to check.\n",
    "    - valid_attribute_names (List[str]): A list of valid attribute names.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Any]: Dictionary containing unexpected attributes and their details.\n",
    "    \"\"\"\n",
    "    unexpected_attributes = {}\n",
    "    total_attributes_checked = 0\n",
    "    objects_with_issues = []\n",
    "\n",
    "    for obj_index, json_object in enumerate(source_objects):\n",
    "        attributes = json_object.get(\"attributes\", {}).get(\"data\", [])\n",
    "        object_unexpected = []\n",
    "\n",
    "        for attr_index, attribute in enumerate(attributes):\n",
    "            total_attributes_checked += 1\n",
    "            attribute_name = attribute.get(\"attributeName\")\n",
    "\n",
    "            if attribute_name not in valid_attribute_names:\n",
    "                if attribute_name not in unexpected_attributes:\n",
    "                    unexpected_attributes[attribute_name] = {\"count\": 0, \"objects\": []}\n",
    "\n",
    "                unexpected_attributes[attribute_name][\"count\"] += 1\n",
    "                unexpected_attributes[attribute_name][\"objects\"].append(\n",
    "                    {\n",
    "                        \"object_index\": obj_index,\n",
    "                        \"attribute_index\": attr_index,\n",
    "                        \"object_id\": json_object.get(\"id\", \"unknown\"),\n",
    "                    }\n",
    "                )\n",
    "                object_unexpected.append(attribute_name)\n",
    "\n",
    "        if object_unexpected:\n",
    "            objects_with_issues.append(\n",
    "                {\n",
    "                    \"object_index\": obj_index,\n",
    "                    \"object_id\": json_object.get(\"id\", \"unknown\"),\n",
    "                    \"unexpected_attributes\": object_unexpected,\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"unexpected_attributes\": unexpected_attributes,\n",
    "        \"objects_with_issues\": objects_with_issues,\n",
    "        \"total_attributes_checked\": total_attributes_checked,\n",
    "        \"total_objects_checked\": len(source_objects),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6845f304",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_fuzzy_matching(\n",
    "    unexpected_attributes: Dict[str, Any], valid_attribute_names: List[str]\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Adds fuzzy matching suggestions to unexpected attributes.\n",
    "\n",
    "    Args:\n",
    "    - unexpected_attributes (Dict[str, Any]): Dictionary of unexpected attributes to enhance.\n",
    "    - valid_attribute_names (List[str]): List of valid attribute names for matching.\n",
    "    \"\"\"\n",
    "\n",
    "    for unexpected_name in unexpected_attributes.keys():\n",
    "        similar_names = get_close_matches(\n",
    "            unexpected_name, valid_attribute_names, n=3, cutoff=0.6\n",
    "        )\n",
    "        unexpected_attributes[unexpected_name][\"similar_valid_names\"] = similar_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96b2964d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analysis_report(\n",
    "    analysis_results: Dict[str, Any],\n",
    "    attribute_report_path: str,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Saves the analysis results to a YAML file.\n",
    "\n",
    "    Args:\n",
    "    - analysis_results (Dict[str, Any]): Results from the attribute analysis.\n",
    "    - attribute_report_path (str): Path to save the report.\n",
    "\n",
    "    Returns:\n",
    "    - bool: True if successful, False otherwise.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        report_data = {\n",
    "            \"analysis_summary\": {\n",
    "                \"total_objects_checked\": analysis_results[\"total_objects_checked\"],\n",
    "                \"objects_with_issues\": len(analysis_results[\"objects_with_issues\"]),\n",
    "                \"unique_unexpected_attributes\": len(\n",
    "                    analysis_results[\"unexpected_attributes\"]\n",
    "                ),\n",
    "            },\n",
    "            \"unexpected_attribute_names\": list(\n",
    "                analysis_results[\"unexpected_attributes\"].keys()\n",
    "            ),\n",
    "            \"detailed_findings\": analysis_results[\"unexpected_attributes\"],\n",
    "            \"affected_objects\": analysis_results[\"objects_with_issues\"],\n",
    "        }\n",
    "\n",
    "        with open(attribute_report_path, \"w\") as file:\n",
    "            yaml.dump(report_data, file, default_flow_style=False)\n",
    "        logger.info(f\"Analysis report saved to '{attribute_report_path}'\")\n",
    "        return True\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error saving analysis report: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7177e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_unexpected_attribute_names(\n",
    "    source_objects: List[Dict[str, Any]],\n",
    "    valid_attribute_names: List[str],\n",
    "    attribute_report_path: str,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Main function to detect unexpected attribute names with comprehensive reporting.\n",
    "\n",
    "    Args:\n",
    "    - source_objects (List[Dict[str, Any]]): A list of JSON objects to check.\n",
    "    - valid_attribute_names (List[str]): A list of valid attribute names.\n",
    "    - attribute_report_path (str): Path to save the detailed report.\n",
    "\n",
    "    Returns:\n",
    "    - Dict[str, Any]: Summary of findings.\n",
    "    \"\"\"\n",
    "    # Capture unexpected attributes\n",
    "    analysis_results = capture_unexpected_attributes(\n",
    "        source_objects, valid_attribute_names\n",
    "    )\n",
    "\n",
    "    # If unexpected attributes found, enhance with fuzzy matching\n",
    "    if analysis_results[\"unexpected_attributes\"]:\n",
    "        add_fuzzy_matching(\n",
    "            analysis_results[\"unexpected_attributes\"], valid_attribute_names\n",
    "        )\n",
    "\n",
    "        # Log findings\n",
    "        logger.warning(\n",
    "            f\"Found {len(analysis_results['unexpected_attributes'])} unexpected attribute names \"\n",
    "            f\"across {len(analysis_results['objects_with_issues'])} objects\"\n",
    "        )\n",
    "\n",
    "        for attr_name, details in analysis_results[\"unexpected_attributes\"].items():\n",
    "            logger.warning(\n",
    "                f\"Unexpected attribute: '{attr_name}' (found {details['count']} times)\"\n",
    "            )\n",
    "            if details.get(\"similar_valid_names\"):\n",
    "                logger.info(\n",
    "                    f\"  Possible matches: {', '.join(details['similar_valid_names'])}\"\n",
    "                )\n",
    "\n",
    "        # Save detailed report\n",
    "        save_analysis_report(analysis_results, attribute_report_path)\n",
    "    else:\n",
    "        logger.info(\"No unexpected attribute names detected.\")\n",
    "\n",
    "    return {\n",
    "        \"total_objects_checked\": analysis_results[\"total_objects_checked\"],\n",
    "        \"total_attributes_checked\": analysis_results[\"total_attributes_checked\"],\n",
    "        \"objects_with_issues\": len(analysis_results[\"objects_with_issues\"]),\n",
    "        \"unique_unexpected_attributes\": len(analysis_results[\"unexpected_attributes\"]),\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e9a6f2e",
   "metadata": {},
   "source": [
    "### Preprocessor Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "842db1a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_attributes(attributes: Dict[str, Any]) -> None:\n",
    "    \"\"\"Validate attribute structure and values\"\"\"\n",
    "    if not isinstance(attributes, dict):\n",
    "        raise ValueError(\n",
    "            f\"Invalid attribute type: expected dict, got {type(attributes)}\"\n",
    "        )\n",
    "\n",
    "    required_fields = [\"attributeName\", \"attributeValue\"]\n",
    "\n",
    "    if not attributes[\"data\"]:\n",
    "        raise ValueError(f\"Missing `data` in `attributes`\")\n",
    "\n",
    "    attributes_data_keys = attributes[\"data\"][0].keys()\n",
    "    if not all(k in attributes_data_keys for k in required_fields):\n",
    "        missing_fields = [k for k in required_fields if k not in attributes_data_keys]\n",
    "\n",
    "        raise ValueError(f\"Missing required fields: {missing_fields}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3b268b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_acm(acm: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Validate the ACM structure\"\"\"\n",
    "    required_fields = [\"portion\", \"banner\"]\n",
    "\n",
    "    if not all(field in acm for field in required_fields):\n",
    "        missing = [f for f in required_fields if f not in acm]\n",
    "\n",
    "        logger.warning(f\"Missing ACM fields: {missing}\")\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "96624cc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _validate_required_fields(obj: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Validate required object fields\"\"\"\n",
    "    if not obj.get(\"id\"):\n",
    "        logger.warning(\"Raw object is missing 'id' attribute\")\n",
    "        return False\n",
    "\n",
    "    if not _validate_acm(obj.get(\"acm\", {})):\n",
    "        logger.error(f\"Failed ACM validation for object {obj.get('id')}\")\n",
    "        return False\n",
    "\n",
    "    if not obj.get(\"attributes\"):\n",
    "        logger.warning(f\"No attributes found for object {obj.get('id')}\")\n",
    "        return False\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ebc9cb2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_cases_raw(raw_object: Dict[str, Any]) -> None:\n",
    "    \"\"\"\n",
    "    Handle special cases for attributes that need custom processing logic on raw objects.\n",
    "\n",
    "    This function modifies the raw object's attributes in place to handle:\n",
    "    - Target Restriction: Convert to boolean\n",
    "    - Military Symbology Code: Validate length and nullify if invalid\n",
    "\n",
    "    Args:\n",
    "        raw_object: The raw object containing attributes.data to process\n",
    "    \"\"\"\n",
    "    try:\n",
    "        attributes_data = raw_object.get(\"attributes\", {}).get(\"data\", [])\n",
    "\n",
    "        # Use a list to track attributes to remove (to avoid modifying list during iteration)\n",
    "        attributes_to_remove = []\n",
    "\n",
    "        for i, attr in enumerate(attributes_data):\n",
    "            attr_name = attr.get(\"attributeName\")\n",
    "            attr_value = attr.get(\"attributeValue\")\n",
    "\n",
    "            if not attr_name:\n",
    "                continue\n",
    "\n",
    "            # Handle Target Restriction - ensure boolean value\n",
    "            if attr_name == \"Target Restriction\":\n",
    "                if attr_value is not None:\n",
    "                    attr[\"attributeValue\"] = bool(attr_value)\n",
    "                    logger.debug(\n",
    "                        f\"Converted Target Restriction to boolean: {attr['attributeValue']}\"\n",
    "                    )\n",
    "\n",
    "            # Handle Military Symbology Code - validate length\n",
    "            elif attr_name == \"Military Symbology Code\":\n",
    "                if attr_value is None or (\n",
    "                    isinstance(attr_value, str) and len(attr_value) != 15\n",
    "                ):\n",
    "                    # Mark this attribute for removal instead of setting to None\n",
    "                    attributes_to_remove.append(i)\n",
    "                    logger.debug(\n",
    "                        f\"Removing invalid Military Symbology Code (length: {len(attr_value) if attr_value else 'None'})\"\n",
    "                    )\n",
    "\n",
    "        # Remove invalid attributes in reverse order to maintain indices\n",
    "        for i in reversed(attributes_to_remove):\n",
    "            attributes_data.pop(i)\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Error handling special cases for object {raw_object.get('id', 'unknown')}: {e}\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d778ccbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_raw_data(raw_objects: List[Dict[str, Any]]):\n",
    "    \"\"\"\n",
    "    Preprocess and validate raw objects\n",
    "\n",
    "    Args:\n",
    "        raw_objects: List of raw input objects\n",
    "\n",
    "    Returns:\n",
    "        Dict mapping object IDs to preprocessed objects\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "\n",
    "    for obj in raw_objects:\n",
    "        # 1. First validate required fields\n",
    "        if not _validate_required_fields(obj):\n",
    "            continue\n",
    "\n",
    "        # 2. Then validate attributes\n",
    "        attributes = obj.get(\"attributes\", {})\n",
    "\n",
    "        try:\n",
    "            _validate_attributes(attributes)\n",
    "        except ValueError as e:\n",
    "            logger.error(\n",
    "                f\"Attribute validation failed for object {obj.get('id')}: {str(e)}\"\n",
    "            )\n",
    "            continue\n",
    "\n",
    "        # 3. Handle special cases for attribute processing\n",
    "        handle_special_cases_raw(obj)\n",
    "\n",
    "        # 4. If all validations pass, format and add to processed data\n",
    "        processed_data.append(obj)\n",
    "    return processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe7f4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dates(source_objects: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Converts date strings in 'lastVerified.timestamp' and 'Date Of Introduction' attributes to Unix timestamps.\n",
    "    \"\"\"\n",
    "\n",
    "    def to_unix(date_str: str) -> Optional[int]:\n",
    "        for fmt in (\"%Y-%m-%dT%H:%M:%S.%fZ\", \"%Y-%m-%dT%H:%M:%SZ\", \"%Y-%m-%d\"):\n",
    "            try:\n",
    "                return int(datetime.strptime(date_str, fmt).timestamp())\n",
    "            except Exception:\n",
    "                continue\n",
    "        return None\n",
    "\n",
    "    for obj in source_objects:\n",
    "        # lastVerified.timestamp\n",
    "        ts = obj.get(\"lastVerified\", {}).get(\"timestamp\")\n",
    "        if isinstance(ts, str):\n",
    "            unix_ts = to_unix(ts)\n",
    "            if unix_ts is not None:\n",
    "                obj[\"lastVerified\"][\"timestamp\"] = unix_ts\n",
    "\n",
    "        # TODO: Once I implement the fuzzy matching logic, this can be further simplified\n",
    "        # Date Of Introduction in attributes\n",
    "        for attr in obj.get(\"attributes\", {}).get(\"data\", []):\n",
    "            if attr.get(\"attributeName\", \"\").strip().lower() == \"date of introduction\":\n",
    "                date_str = attr.get(\"attributeValue\")\n",
    "                if isinstance(date_str, str):\n",
    "                    unix_ts = to_unix(date_str)\n",
    "                    if unix_ts is not None:\n",
    "                        attr[\"attributeValue\"] = unix_ts\n",
    "\n",
    "    logger.info(\"Dates prepared successfully.\")\n",
    "    return source_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd96126",
   "metadata": {},
   "source": [
    "### Classification Restriction Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ce86fc38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_classif_too_high(ism: Dict[str, Any], config: Dict[str, Any]) -> bool:\n",
    "    \"\"\"Return True if ISM is too highly classified or contains forbidden controls/terms.\"\"\"\n",
    "    if not ism:\n",
    "        logger.warning(\"ISM is empty, cannot determine classification level.\")\n",
    "        return False\n",
    "\n",
    "    if (\n",
    "        ism.get(\"classification\") == \"TS\"\n",
    "        or set(ism.get(\"sciControls\", [])) & set(config[\"forbidden_sci\"])\n",
    "        or set(ism.get(\"disseminationControls\", [])) & set(config[\"forbidden_controls\"])\n",
    "        or any(\n",
    "            term in ism.get(\"banner\", \"\").upper() for term in config[\"forbidden_terms\"]\n",
    "        )\n",
    "    ):\n",
    "        # logger.warning(f\"ISM too high or contains forbidden values: {ism}\")\n",
    "        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e52b4a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_more_restrictive(\n",
    "    ism1: Dict[str, Any], ism2: Dict[str, Any], config: Dict[str, Any]\n",
    ") -> bool:\n",
    "    \"\"\"Return True if ism1 is more restrictive than ism2.\"\"\"\n",
    "    if not ism1 or not ism2:\n",
    "        return False\n",
    "\n",
    "    # FGI controls\n",
    "    ism1_fgi = any(c.startswith(\"FGI\") for c in ism1.get(\"sciControls\", []))\n",
    "    ism2_fgi = any(c.startswith(\"FGI\") for c in ism2.get(\"sciControls\", []))\n",
    "    if ism1_fgi != ism2_fgi:\n",
    "        return ism1_fgi\n",
    "\n",
    "    # NOFORN\n",
    "    ism1_noforn = \"NOFORN\" in ism1.get(\"disseminationControls\", [])\n",
    "    ism2_noforn = \"NOFORN\" in ism2.get(\"disseminationControls\", [])\n",
    "    if ism1_noforn != ism2_noforn:\n",
    "        return ism1_noforn\n",
    "\n",
    "    # REL controls\n",
    "    rel1 = \"REL\" in ism1.get(\"disseminationControls\", [])\n",
    "    rel2 = \"REL\" in ism2.get(\"disseminationControls\", [])\n",
    "    if rel1 and rel2:\n",
    "        ism1_release = set(ism1.get(\"releasableTo\", []))\n",
    "        ism2_release = set(ism2.get(\"releasableTo\", []))\n",
    "        ism1_groups = ism1_release & set(config[\"special_groups\"])\n",
    "        ism2_groups = ism2_release & set(config[\"special_groups\"])\n",
    "        # More restrictive if ism1 has fewer groups or fewer releasable entities\n",
    "        if ism1_groups != ism2_groups:\n",
    "            return len(ism1_groups) < len(ism2_groups)\n",
    "        return len(ism1_release) < len(ism2_release)\n",
    "\n",
    "    # Classification hierarchy\n",
    "    classif1 = ism1.get(\"classification\", \"U\")\n",
    "    classif2 = ism2.get(\"classification\", \"U\")\n",
    "    return config[\"classifications\"].get(classif1, 0) > config[\"classifications\"].get(\n",
    "        classif2, 0\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da1fcb63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_most_restrictive_valid_ism(\n",
    "    obj: Dict[str, Any], config: Dict[str, Any]\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Traverse a nested object to find the most restrictive valid ISM (Information Security Marking).\n",
    "\n",
    "    The function searches through all dictionaries and lists within the provided object,\n",
    "    identifies ISMs that are not too highly classified (using is_classif_too_high),\n",
    "    and returns the most restrictive valid ISM according to the is_more_restrictive function.\n",
    "\n",
    "    Args:\n",
    "        obj (Dict[str, Any]): The object to search for ISMs.\n",
    "        config (Dict[str, Any]): The classification configuration dictionary.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Dict[str, Any]]: The most restrictive valid ISM found, or None if no valid ISM exists.\n",
    "    \"\"\"\n",
    "    most_restrictive = None\n",
    "    stack = [obj]  # Use a stack to traverse the object hierarchy\n",
    "\n",
    "    while stack:\n",
    "        item = stack.pop()\n",
    "\n",
    "        if isinstance(item, dict):\n",
    "            # Check if the current item has an ISM and if it's valid\n",
    "            if \"ism\" in item:\n",
    "                ism = item.get(\"ism\")\n",
    "                if ism and not is_classif_too_high(ism, config):\n",
    "                    # Early exit if 'TS' found\n",
    "                    if ism.get(\"classification\") == \"TS\":\n",
    "                        return ism.copy()\n",
    "                    if most_restrictive is None or is_more_restrictive(\n",
    "                        ism, most_restrictive, config\n",
    "                    ):\n",
    "                        most_restrictive = ism.copy()\n",
    "\n",
    "            # Add all dictionary values to the stack\n",
    "            stack.extend(item.values())\n",
    "        elif isinstance(item, list):\n",
    "            # Add all list items to the stack\n",
    "            stack.extend(item)\n",
    "\n",
    "    # Print the most restrictive ISM found\n",
    "    # logger.info(f\"Most restrictive valid ISM found: {most_restrictive}\")\n",
    "    return most_restrictive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "22098a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_restrictions(\n",
    "    standard_object: Dict[str, Any], config: Dict[str, Any]\n",
    ") -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Recursively process a standard object to remove or redact data that is too highly classified,\n",
    "    according to the provided classification configuration.\n",
    "\n",
    "    The function finds the most restrictive valid ISM (Information Security Marking) within the object,\n",
    "    then traverses all nested dictionaries and lists, replacing any data with a classification that is\n",
    "    considered too high with a placeholder. The processed object will include an 'overallClassification'\n",
    "    field set to the most restrictive valid ISM found.\n",
    "\n",
    "    Args:\n",
    "        standard_object (Dict[str, Any]): The object to process and apply restrictions to.\n",
    "        config (Dict[str, Any]): The classification configuration dictionary.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Dict[str, Any]]: The processed object with restricted data redacted, or None if no valid ISM is found.\n",
    "    \"\"\"\n",
    "    # Find the most restrictive valid ISM in the object\n",
    "    most_restrictive_ism = find_most_restrictive_valid_ism(standard_object, config)\n",
    "\n",
    "    if not most_restrictive_ism:\n",
    "        # If no valid ISM is found, return None\n",
    "        logger.warning(\"No valid ISM found for object\")\n",
    "        return None\n",
    "\n",
    "    def process_item(item: Any) -> Any:\n",
    "        if isinstance(item, dict):\n",
    "            # If the item has an ISM, check if it is too high\n",
    "            if \"ism\" in item and is_classif_too_high(item[\"ism\"], config):\n",
    "                logger.debug(f\"Removing item due to high classification: {item}\")\n",
    "                return None\n",
    "\n",
    "            # Process all key-value pairs in the dictionary\n",
    "            return {k: process_item(v) for k, v in item.items()}\n",
    "        if isinstance(item, list):\n",
    "            # Process all items in the list\n",
    "            return [process_item(x) for x in item]\n",
    "\n",
    "        return item  # Return the item as is if it's neither a dict nor a list\n",
    "\n",
    "    # Process the object and add the overall classification\n",
    "    processed_object = process_item(standard_object)\n",
    "    if isinstance(processed_object, dict):\n",
    "        processed_object[\"overallClassification\"] = most_restrictive_ism\n",
    "\n",
    "    return processed_object"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e76a1a",
   "metadata": {},
   "source": [
    "### Validate the final standard object against the schema\n",
    "* Simply run a function that makes sure the final parsed object conforms to the standard_object_schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1aae57c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_standard_object(\n",
    "    standard_object: dict,\n",
    "    schema: dict,\n",
    ") -> bool:\n",
    "    \"\"\"\n",
    "    Validate a standard object against the JSON schema with detailed error reporting.\n",
    "\n",
    "    Args:\n",
    "        standard_object (dict): The processed standard object to validate\n",
    "        schema_path (str): Path to the JSON schema file\n",
    "\n",
    "    Returns:\n",
    "        bool: True if validation passes, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Validate the object\n",
    "        validate(instance=standard_object, schema=schema)\n",
    "\n",
    "        logger.info(\"VALIDATION PASSED: Standard object conforms to schema\")\n",
    "        logger.info(f\"   - Object ID: {standard_object.get('id', 'Unknown')}\")\n",
    "        return True\n",
    "\n",
    "    except ValidationError as e:\n",
    "        logger.error(\"VALIDATION FAILED: Object does not conform to schema\")\n",
    "        logger.error(f\"Object ID: {standard_object.get('id', 'Unknown')}\")\n",
    "        logger.error(\n",
    "            f\"   Error Located: {' -> '.join(str(x) for x in e.absolute_path) if e.absolute_path else 'Root level'}\"\n",
    "        )\n",
    "        logger.error(f\"   Error Message: {e.message}\")\n",
    "\n",
    "        # Try to provide more context about the failing value\n",
    "        if e.absolute_path:\n",
    "            failing_value = standard_object\n",
    "            try:\n",
    "                for path_element in e.absolute_path:\n",
    "                    failing_value = failing_value[path_element]\n",
    "                logger.warning(f\"   Failing Value: {failing_value}\")\n",
    "                logger.warning(f\"   Value Type: {type(failing_value).__name__}\")\n",
    "            except (KeyError, TypeError, IndexError):\n",
    "                logger.error(\"   Could not retrieve failing value\")\n",
    "\n",
    "        # Show the schema requirement that failed\n",
    "        if hasattr(e, \"schema\"):\n",
    "            schema_info = e.schema\n",
    "            if isinstance(schema_info, dict):\n",
    "                if \"type\" in schema_info:\n",
    "                    logger.warning(f\"   Expected Type: {schema_info['type']}\")\n",
    "                if \"required\" in schema_info:\n",
    "                    logger.warning(f\"   Required Fields: {schema_info['required']}\")\n",
    "\n",
    "        return False\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"VALIDATION FAILED: Unexpected error during validation\")\n",
    "        logger.error(f\"   Error Type: {type(e).__name__}\")\n",
    "        logger.error(f\"   Error Message: {str(e)}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73a58c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_validations(\n",
    "    cleaned_objects: List[Dict[str, Any]],\n",
    "    schema_path: str,\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Validate all standard objects against the JSON schema.\n",
    "\n",
    "    Args:\n",
    "        cleaned_objects: List of cleaned standard objects to validate\n",
    "        schema_path: Path to the JSON schema file\n",
    "\n",
    "    Returns:\n",
    "        Dict containing validation summary and details\n",
    "    \"\"\"\n",
    "    schema = load_standard_object_schema(schema_path)\n",
    "    if not schema:\n",
    "        logger.warning(\"Cannot validate objects without a valid schema.\")\n",
    "        return {\n",
    "            \"all_valid\": False,\n",
    "            \"total_objects\": 0,\n",
    "            \"valid_count\": 0,\n",
    "            \"failed_count\": 0,\n",
    "            \"failed_objects\": [],\n",
    "        }\n",
    "\n",
    "    if not cleaned_objects:\n",
    "        logger.warning(\"No standard objects to validate.\")\n",
    "        return {\n",
    "            \"all_valid\": False,\n",
    "            \"total_objects\": 0,\n",
    "            \"valid_count\": 0,\n",
    "            \"failed_count\": 0,\n",
    "            \"failed_objects\": [],\n",
    "        }\n",
    "\n",
    "    logger.info(f\"Validating {len(cleaned_objects)} standard objects...\")\n",
    "\n",
    "    validation_results = []\n",
    "    failed_objects = []\n",
    "\n",
    "    for i, obj in enumerate(cleaned_objects):\n",
    "        try:\n",
    "            is_valid = validate_standard_object(obj, schema)\n",
    "            validation_results.append(is_valid)\n",
    "\n",
    "            if not is_valid:\n",
    "                failed_objects.append(\n",
    "                    {\n",
    "                        \"index\": i,\n",
    "                        \"id\": obj.get(\"id\", \"Unknown\"),\n",
    "                        \"name\": obj.get(\"name\", \"Unknown\"),\n",
    "                    }\n",
    "                )\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Unexpected error validating object {i}: {e}\")\n",
    "            validation_results.append(False)\n",
    "            failed_objects.append(\n",
    "                {\n",
    "                    \"index\": i,\n",
    "                    \"id\": obj.get(\"id\", \"Unknown\"),\n",
    "                    \"name\": obj.get(\"name\", \"Unknown\"),\n",
    "                    \"error\": str(e),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    # Calculate summary\n",
    "    total_objects = len(cleaned_objects)\n",
    "    valid_count = sum(validation_results)\n",
    "    failed_count = len(failed_objects)\n",
    "    all_valid = all(validation_results)\n",
    "\n",
    "    # Log summary\n",
    "    if all_valid:\n",
    "        logger.info(\n",
    "            f\"ALL VALIDATION PASSED: {total_objects}/{total_objects} objects conform to schema\"\n",
    "        )\n",
    "        logger.info(\"Ready to proceed with all valid standard objects.\")\n",
    "    else:\n",
    "        logger.warning(f\" VALIDATION SUMMARY:\")\n",
    "        logger.warning(f\"   Total objects: {total_objects}\")\n",
    "        logger.warning(f\"   Valid objects: {valid_count}\")\n",
    "        logger.warning(f\"   Failed objects: {failed_count}\")\n",
    "        logger.warning(f\"   Success rate: {(valid_count/total_objects)*100:.1f}%\")\n",
    "\n",
    "        # Log details of failed objects (limit to first 10 to avoid spam)\n",
    "        max_display = 10\n",
    "        logger.warning(\n",
    "            f\"Failed object details (showing first {min(failed_count, max_display)}):\"\n",
    "        )\n",
    "        for i, failed in enumerate(failed_objects[:max_display]):\n",
    "            error_msg = (\n",
    "                f\" - Error: {failed.get('error', 'Schema validation failed')}\"\n",
    "                if \"error\" in failed\n",
    "                else \"\"\n",
    "            )\n",
    "            logger.warning(\n",
    "                f\"   {i+1}. Index {failed['index']}: {failed['id']} ({failed['name']}){error_msg}\"\n",
    "            )\n",
    "\n",
    "        if failed_count > max_display:\n",
    "            logger.warning(\n",
    "                f\"   ... and {failed_count - max_display} more failed objects\"\n",
    "            )\n",
    "\n",
    "    return {\n",
    "        \"all_valid\": all_valid,\n",
    "        \"total_objects\": total_objects,\n",
    "        \"valid_count\": valid_count,\n",
    "        \"failed_count\": failed_count,\n",
    "        \"failed_objects\": failed_objects,\n",
    "        \"validation_results\": validation_results,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a9dcb6",
   "metadata": {},
   "source": [
    "### Parser Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e82a7cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_empty_container(container: Any) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a container is empty.\n",
    "\n",
    "    Args:\n",
    "        container (Any): The container to check\n",
    "\n",
    "    Returns:\n",
    "        bool: True if the container is empty or None, False otherwise\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if container is None:\n",
    "            return True\n",
    "        if isinstance(container, dict):\n",
    "            return all(is_empty_container(value) for value in container.values())\n",
    "        if isinstance(container, list):\n",
    "            return all(is_empty_container(item) for item in container)\n",
    "        return False  # For other types, consider them non-empty if they are not None\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error checking if container is empty: {e}\")\n",
    "        raise ValueError(\"Invalid container structure\")\n",
    "\n",
    "\n",
    "def fix_container_types(objects: List[Dict[str, Any]]) -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Fix container types to match schema requirements.\n",
    "\n",
    "    Converts dict containers to arrays where required by the schema.\n",
    "    Currently handles: provenance (dict -> array)\n",
    "\n",
    "    Args:\n",
    "        objects: List of standard objects to fix\n",
    "\n",
    "    Returns:\n",
    "        List of objects with corrected container types\n",
    "    \"\"\"\n",
    "    # Define containers that should be arrays in the schema\n",
    "    array_containers = {\"location\", \"equipment\", \"provenance\"}  # Add more as needed\n",
    "\n",
    "    for obj in objects:\n",
    "        for container_name in array_containers:\n",
    "            if container_name in obj and isinstance(obj[container_name], dict):\n",
    "                # Convert dict to array format\n",
    "                if obj[container_name]:  # If not empty dict\n",
    "                    obj[container_name] = [obj[container_name]]\n",
    "                else:  # If empty dict\n",
    "                    obj[container_name] = []\n",
    "\n",
    "    return objects\n",
    "\n",
    "\n",
    "def clean_object(obj: Dict[str, Any]) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Remove empty containers from the object.\n",
    "\n",
    "    Args:\n",
    "        obj: The object to clean\n",
    "\n",
    "    Returns:\n",
    "        The cleaned object with empty containers removed\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if isinstance(obj, dict):\n",
    "            # Use dictionary comprehension to clean nested dictionaries\n",
    "            return {\n",
    "                key: clean_object(value)\n",
    "                for key, value in obj.items()\n",
    "                if not is_empty_container(value)\n",
    "            }\n",
    "        elif isinstance(obj, list):\n",
    "            # Use list comprehension to clean nested lists\n",
    "            return [clean_object(item) for item in obj if not is_empty_container(item)]\n",
    "        else:\n",
    "            # return non-container value as is.\n",
    "            return obj\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error cleaning object: {e}\")\n",
    "        raise ValueError(\"An error occurred while cleaning the object\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3c910eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This might fail. If it does, then i will probably need to do something similar to fix_container_types\n",
    "def extract_ism(acm: dict) -> dict:\n",
    "    \"\"\"Extract the reduced 'ism' structure from any ACM dict.\"\"\"\n",
    "    return {\n",
    "        \"banner\": acm.get(\"banner\"),\n",
    "        \"classification\": acm.get(\"classif\"),\n",
    "        \"ownerProducer\": acm.get(\"owner_prod\", []),\n",
    "        \"releaseableTo\": acm.get(\"rel_to\", []),\n",
    "        \"disseminationControls\": acm.get(\"dissem_ctrls\", []),\n",
    "        \"sciControls\": acm.get(\"f_clearance\", []),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "09ad9b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_created_date(source_object: Dict[str, Any]) -> Optional[int]:\n",
    "    \"\"\"\n",
    "    Extracts the 'createdDate' (Unix timestamp) from the source object's attributes.\n",
    "\n",
    "    Args:\n",
    "        source_object (Dict[str, Any]): The source dictionary containing object data.\n",
    "\n",
    "    Returns:\n",
    "        Optional[int]: The Unix timestamp of 'Date Of Introduction' if found, otherwise None.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for attr in source_object.get(\"attributes\", {}).get(\"data\", []):\n",
    "            name = attr.get(\"attributeName\", \"\").strip().lower()\n",
    "            if name == \"date of introduction\":\n",
    "                value = attr.get(\"attributeValue\")\n",
    "                if isinstance(value, int):\n",
    "                    return value\n",
    "    except Exception as e:\n",
    "        logger.error(f\"There was an error extracting createdDate: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4fcf6dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: This will have to be updated once I implement the fuzzy matching logic\n",
    "def extract_elevation(source_object: Dict[str, Any]) -> Optional[Any]:\n",
    "    \"\"\"\n",
    "    Retrieves the elevation value from the source object, handling variations\n",
    "    in the attribute name (e.g., \"Elevation\", \"Elevation(m)\", \"Elevation (m)\").\n",
    "\n",
    "    Args:\n",
    "        source_object (Dict[str, Any]): The source JSON-like object.\n",
    "\n",
    "    Returns:\n",
    "        Optional[Any]: The elevation value if found, otherwise None.\n",
    "    \"\"\"\n",
    "    elevation_value = None\n",
    "\n",
    "    # Define possible variations of the \"Elevation\" attribute name\n",
    "    elevation_variations = [\"elevation\", \"elevation(m)\", \"elevation (m)\"]\n",
    "\n",
    "    try:\n",
    "        # Ensure the source object is a dictionary and contains the expected structure\n",
    "        if not isinstance(source_object, dict):\n",
    "            raise ValueError(\"source object must be a dictionary.\")\n",
    "\n",
    "        if (\n",
    "            \"attributes\" not in source_object\n",
    "            or \"data\" not in source_object[\"attributes\"]\n",
    "        ):\n",
    "            raise KeyError(\n",
    "                \"source object does not contain the expected 'attributes.data' structure.\"\n",
    "            )\n",
    "\n",
    "        # Iterate through the attributes to find the elevation value\n",
    "        for attr in source_object[\"attributes\"][\"data\"]:\n",
    "            attribute_name = attr.get(\"attributeName\", \"\").lower()\n",
    "\n",
    "            if (\n",
    "                attribute_name in elevation_variations\n",
    "                and attr.get(\"attributeValue\") is not None\n",
    "            ):\n",
    "                elevation_value = attr.get(\"attributeValue\")\n",
    "                break  # Exit the loop once the elevation value is found\n",
    "\n",
    "        if isinstance(elevation_value, str):\n",
    "            try:\n",
    "                elevation_value = float(elevation_value)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error transforming elevation into float: {e}\")\n",
    "    except Exception as e:\n",
    "        # Log the exception for debugging purposes\n",
    "        logger.error(f\"Error occurred while retrieving elevation: {e}\")\n",
    "\n",
    "    return elevation_value\n",
    "\n",
    "\n",
    "def extract_desired_attribute(\n",
    "    source_object: Dict[str, Any], attribute_name: str\n",
    ") -> Optional[Any]:\n",
    "    attribute_value = None\n",
    "\n",
    "    try:\n",
    "        # Ensure the source object is a dictionary and contains the expected structure\n",
    "        if not isinstance(source_object, dict):\n",
    "            raise ValueError(\"source object must be a dictionary.\")\n",
    "\n",
    "        if (\n",
    "            \"attributes\" not in source_object\n",
    "            or \"data\" not in source_object[\"attributes\"]\n",
    "        ):\n",
    "            raise KeyError(\n",
    "                \"source object does not contain the expected 'attributes.data' structure.\"\n",
    "            )\n",
    "\n",
    "        # Iterate through the attributes to find the desired attribute\n",
    "        for attr in source_object[\"attributes\"][\"data\"]:\n",
    "            if (\n",
    "                attr.get(\"attributeName\") == attribute_name\n",
    "                and attr.get(\"attributeValue\") is not None\n",
    "            ):\n",
    "                attribute_value = attr.get(\"attributeValue\")\n",
    "                break  # Exit the loop once the desired attribute is found\n",
    "\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error occurred while retrieving {attribute_name}: {e}\")\n",
    "\n",
    "    return attribute_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "85e1cf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_attribute_index(source: Dict[str, Any]) -> Dict[str, Dict]:\n",
    "    \"\"\"\n",
    "    Prepare a complete attribute index including both standard attributes and top-level fields.\n",
    "\n",
    "    Args:\n",
    "        source: Source dictionary containing object data\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Dict]: Attribute index with both regular attributes and transformed top-level fields\n",
    "    \"\"\"\n",
    "    # Get standard attributes from data items\n",
    "    data_items = source.get(\"attributes\", {}).get(\"data\", [])\n",
    "    attr_index = {item.get(\"attributeName\"): item for item in data_items}\n",
    "\n",
    "    # Define top-level fields to be included in attribute mapping\n",
    "    top_level_fields = {\n",
    "        \"domain\": \"Domain\",\n",
    "        \"allegiance\": \"Allegiance\",\n",
    "        \"allegianceAor\": \"Allegiance Aor\",\n",
    "        \"eoid\": \"Enterprise Object ID\",\n",
    "    }\n",
    "\n",
    "    # Add top-level fields to attribute index with proper structure\n",
    "    for source_field, attr_name in top_level_fields.items():\n",
    "        if source_field in source:\n",
    "            attr_index[attr_name] = {\n",
    "                \"attributeValue\": source[source_field],\n",
    "                \"acm\": source.get(\"acm\", {}),\n",
    "            }\n",
    "\n",
    "    return attr_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7259309",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_location(source_object: Dict[str, Any]) -> Optional[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Processes location information from the input object's geographic data.\n",
    "\n",
    "    Args:\n",
    "        source_object (Dict[str, Any]): The input object containing location data.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: Processed location data, or None if data is invalid.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        location_data = source_object.get(\"latestKnownLocation\")\n",
    "        if not location_data:\n",
    "            logger.warning(\n",
    "                f\"No location data found for object {source_object.get('id')}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        geometry_data = location_data.get(\"geometry\")\n",
    "        if not geometry_data:\n",
    "            logger.warning(\n",
    "                f\"No geometry data found for object {source_object.get('id')}\"\n",
    "            )\n",
    "            return None\n",
    "\n",
    "        # Enhance coordinate validation - accept arrays with 2 or more values\n",
    "        coords = geometry_data.get(\"coordinates\", [])\n",
    "        if not coords or len(coords) < 2:\n",
    "            logger.warning(f\"Invalid coordinates for object {source_object.get('id')}\")\n",
    "            return None\n",
    "\n",
    "        if len(coords) > 2:\n",
    "            logger.warning(\n",
    "                f\"Using first two values from {len(coords)}-element coordinate array for object {source_object.get('id')}.\"\n",
    "            )\n",
    "\n",
    "        elevation_value = extract_elevation(source_object)\n",
    "        if isinstance(elevation_value, str):\n",
    "            try:\n",
    "                elevation_value = float(elevation_value)\n",
    "            except Exception as e:\n",
    "                logger.error(f\"Error transforming elevation into float: {e}\")\n",
    "\n",
    "        return {\n",
    "            \"ism\": extract_ism(location_data.get(\"acm\", {})),\n",
    "            \"id\": location_data.get(\"id\"),\n",
    "            \"timestamp\": location_data.get(\"lastVerified\", {}).get(\"timestamp\"),\n",
    "            \"latitude\": coords[1],\n",
    "            \"longitude\": coords[0],\n",
    "            \"altitude\": {\n",
    "                \"value\": None,\n",
    "                \"quality\": None,\n",
    "                \"error\": None,\n",
    "                \"units\": {\"value\": None},\n",
    "            },\n",
    "            \"elevation\": {\n",
    "                \"value\": elevation_value,\n",
    "                \"quality\": None,\n",
    "                \"error\": None,\n",
    "                \"units\": {\"value\": None},\n",
    "            },\n",
    "            \"derivation\": geometry_data.get(\"type\"),\n",
    "            \"quality\": None,\n",
    "            \"locationName\": None,\n",
    "            \"countryCode\": None,\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Unexpected error writing location values for object {source_object.get('id')}: {str(e)}\"\n",
    "        )\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e878c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_ship_class_name(\n",
    "    source_object: Dict[str, Any], standard_object: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Updates the standard_object's maritimeMetadata with shipClass and shipName if the object is a ship.\n",
    "\n",
    "    Args:\n",
    "        source_object (Dict[str, Any]): Input dictionary containing vessel information.\n",
    "        standard_object (Dict[str, Any]): Dictionary to be updated with vessel metadata.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The updated standard_object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        attributes = source_object.get(\"attributes\", {}).get(\"data\", [])\n",
    "        class_name = source_object.get(\"className\")\n",
    "        acm = source_object.get(\"acm\", {})\n",
    "\n",
    "        # Using any() to more efficiently determine shipName-shipClass without a for loop\n",
    "        is_ship = any(\n",
    "            attr.get(\"attributeName\") == \"Echelon\"\n",
    "            and attr.get(\"attributeValue\") == \"SHIP\"\n",
    "            for attr in attributes\n",
    "        )\n",
    "        # Find shipName and its ACM from the attribute\n",
    "        ship_name_attr = next(\n",
    "            (attr for attr in attributes if attr.get(\"attributeName\") == \"Name\"),\n",
    "            None,\n",
    "        )\n",
    "\n",
    "        ship_name = ship_name_attr.get(\"attributeValue\") if ship_name_attr else None\n",
    "        ship_name_acm = ship_name_attr.get(\"acm\", {}) if ship_name_attr else {}\n",
    "\n",
    "        if is_ship:\n",
    "            if \"maritimeMetadata\" not in standard_object or not isinstance(\n",
    "                standard_object[\"maritimeMetadata\"], dict\n",
    "            ):\n",
    "                standard_object[\"maritimeMetadata\"] = {}\n",
    "\n",
    "            standard_object[\"maritimeMetadata\"][\"shipClass\"] = {\n",
    "                \"value\": class_name,\n",
    "                \"ism\": extract_ism(acm),\n",
    "            }\n",
    "            if ship_name:\n",
    "                standard_object[\"maritimeMetadata\"][\"shipName\"] = {\n",
    "                    \"value\": ship_name,\n",
    "                    \"ism\": extract_ism(ship_name_acm),\n",
    "                }\n",
    "\n",
    "            logger.info(\n",
    "                f\"Set shipClass and shipName for object {standard_object.get('id')}\"\n",
    "            )\n",
    "        return standard_object\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Error parsing ship class/name for object {standard_object.get('id', 'unknown')}: {e}\"\n",
    "        )\n",
    "        return standard_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166cf4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_facility_name_id(\n",
    "    source_object: Dict[str, Any], standard_object: Dict[str, Any]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Updates the standard_object with facilityName and facilityId if the object represents a facility.\n",
    "\n",
    "    Args:\n",
    "        source_object (Dict[str, Any]): Dictionary containing facility information.\n",
    "        standard_object (Dict[str, Any]): Dictionary to be updated with facility metadata.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The updated standard_object.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        attributes = source_object.get(\"attributes\", {}).get(\"data\", [])\n",
    "        class_name = source_object.get(\"className\")\n",
    "        acm = source_object.get(\"acm\", {})\n",
    "\n",
    "        is_facility = class_name == \"Facility\"\n",
    "        facility_name = source_object.get(\"name\")\n",
    "\n",
    "        # Find OSuffix attribute and its ACM\n",
    "        facility_id_attr = next(\n",
    "            (\n",
    "                attr\n",
    "                for attr in attributes\n",
    "                if attr.get(\"attributeName\") == \"OSuffix\"\n",
    "                and attr.get(\"attributeValue\") is not None\n",
    "            ),\n",
    "            None,\n",
    "        )\n",
    "        facility_id = (\n",
    "            facility_id_attr.get(\"attributeValue\") if facility_id_attr else None\n",
    "        )\n",
    "        facility_id_acm = facility_id_attr.get(\"acm\", {}) if facility_id_attr else {}\n",
    "\n",
    "        if is_facility:\n",
    "            if \"facility\" not in standard_object or not isinstance(\n",
    "                standard_object[\"facility\"], dict\n",
    "            ):\n",
    "                standard_object[\"facility\"] = {}\n",
    "\n",
    "            standard_object[\"facility\"][\"facilityName\"] = {\n",
    "                \"value\": facility_name,\n",
    "                \"ism\": extract_ism(acm),\n",
    "            }\n",
    "            if facility_id:\n",
    "                standard_object[\"facility\"][\"facilityId\"] = {\n",
    "                    \"value\": facility_id,\n",
    "                    \"ism\": extract_ism(facility_id_acm),\n",
    "                }\n",
    "\n",
    "            logger.info(\n",
    "                f\"Set facilityName and facilityId for object {standard_object.get('id')}\"\n",
    "            )\n",
    "        return standard_object\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Error parsing facility name/id for object {standard_object.get('id', 'unknown')}: {e}\"\n",
    "        )\n",
    "        return standard_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323d9190",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_standard_object(\n",
    "    target_structure: Dict[str, Any],\n",
    "    attr_index: Dict[str, Dict],\n",
    "    attribute_map: Dict[str, Dict[str, str]],\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Build a standard object by mapping attributes from the source data to target fields.\n",
    "\n",
    "    This function takes a pre-initialized target_structure dictionary and populates it with\n",
    "    transformed attribute values based on the provided attribute mapping. Each\n",
    "    attribute value is wrapped with ISM classification metadata.\n",
    "\n",
    "    Args:\n",
    "        target_structure (Dict[str, Any]): Pre-initialized dictionary containing basic object metadata\n",
    "            and empty containers (ontology, maritimeMetadata, equipment, facility)\n",
    "\n",
    "        attr_index (Dict[str, Dict]): Index of attribute data items keyed by attribute name,\n",
    "            where each item contains 'attributeValue' and 'acm' fields\n",
    "\n",
    "        attribute_map (Dict[str, Dict[str, str]]): Mapping configuration where keys are attribute names and values are dicts with\n",
    "            'field' and 'container' specifications\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, Any]: The populated target_structure dictionary with mapped attributes organized\n",
    "            into their designated containers, or empty dict if an error occurs\n",
    "\n",
    "    Note:\n",
    "        - Attributes mapped to \"root\" container are placed directly in the target_structure dict\n",
    "        - Other containers are nested under their respective keys\n",
    "        - Each mapped value includes the original value and ISM classification metadata\n",
    "        - Missing attributes in attr_index are silently skipped\n",
    "    \"\"\"\n",
    "    try:\n",
    "        for attr_name, mapping in attribute_map.items():\n",
    "            item = attr_index.get(attr_name)\n",
    "\n",
    "            if not item:\n",
    "                continue\n",
    "\n",
    "            target_field = mapping[\"field\"]\n",
    "            container = mapping[\"container\"]\n",
    "\n",
    "            transformed_value = {\n",
    "                \"value\": item.get(\"attributeValue\"),\n",
    "                \"ism\": extract_ism(item.get(\"acm\", {})),\n",
    "            }\n",
    "\n",
    "            if container == \"root\":\n",
    "                target_structure[target_field] = transformed_value\n",
    "            else:\n",
    "                # Ensure nested container exists\n",
    "                if container not in target_structure:\n",
    "                    target_structure[container] = {}\n",
    "                target_structure[container][target_field] = transformed_value\n",
    "\n",
    "        return target_structure\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error building standard object: {e}\")\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dd2d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_source_object(\n",
    "    source: Dict[str, Any], attribute_map: Dict[str, Dict[str, str]]\n",
    ") -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Transform a source object into a structured format based on the provided attribute mapping.\n",
    "\n",
    "    Args:\n",
    "        source: The source dictionary containing object data with attributes, ACM, and metadata\n",
    "        attribute_map: Dictionary mapping attribute names to their target field and container locations\n",
    "\n",
    "    Returns:\n",
    "        Dict containing the transformed object with structured fields\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if not isinstance(source, dict) or not isinstance(attribute_map, dict):\n",
    "            logger.error(\"Invalid source object or attribute map.\")\n",
    "            return {}\n",
    "\n",
    "        # Special handling for createdDate.\n",
    "        created_date = extract_created_date(source)\n",
    "\n",
    "        # Initialize target structure with basic metadata\n",
    "        target_structure = {\n",
    "            \"version\": source.get(\"version\"),\n",
    "            \"overallClassification\": extract_ism(source.get(\"acm\", {})),\n",
    "            \"id\": source.get(\"id\"),\n",
    "            \"name\": source.get(\"name\"),\n",
    "            \"sourceId\": source.get(\"gideId\"),\n",
    "            \"createdDate\": created_date,\n",
    "            \"lastUpdatedDate\": source.get(\"lastVerified\", {}).get(\"timestamp\"),\n",
    "            \"maritimeMetadata\": {},\n",
    "            \"landMetadata\": {},\n",
    "            \"location\": parse_location(source),\n",
    "            \"equipment\": {},\n",
    "            \"unit\": {},\n",
    "            \"ontology\": {},\n",
    "            \"facility\": {},\n",
    "        }\n",
    "\n",
    "        # Get complete attribute index including top-level fields\n",
    "        attr_index = prepare_attribute_index(source)\n",
    "\n",
    "        # Build and return the standard object\n",
    "        standard_object = build_standard_object(\n",
    "            target_structure, attr_index, attribute_map\n",
    "        )\n",
    "\n",
    "        # Apply the bespoke functions that parse maritime and facility attributes\n",
    "        parse_ship_class_name(source, standard_object)\n",
    "        parse_facility_name_id(source, standard_object)\n",
    "\n",
    "        if standard_object is not None:\n",
    "            logger.info(\n",
    "                f\"Finished transforming object with ID: {standard_object.get('id', 'unknown')}\"\n",
    "            )\n",
    "            return standard_object\n",
    "        else:\n",
    "            logger.warning(\n",
    "                f\"Transformation resulted in None for object with ID: {source.get('id', 'unknown')}\"\n",
    "            )\n",
    "            return {}\n",
    "    except Exception as e:\n",
    "        logger.error(\n",
    "            f\"Error transforming object with ID {source.get('id', 'unknown')}: {e}\"\n",
    "        )\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0774a1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_objects(\n",
    "    source_objects: List[Dict[str, Any]],\n",
    "    attribute_mapping: Dict[str, Dict[str, str]],\n",
    "    restrictions_config: Dict[str, Any],\n",
    ") -> List[Dict[str, Any]]:\n",
    "    \"\"\"\n",
    "    Processes a list of input objects by parsing, validating, and applying ISM policies.\n",
    "\n",
    "    Args:\n",
    "        source_objects (List[Dict[str, Any]]): List of objects to process.\n",
    "        attribute_mapping (Dict[str, Dict[str, str]]): Mapping configuration for attributes.\n",
    "        restrictions_config (Dict[str, Any]): Configuration for classification restrictions.\n",
    "\n",
    "    Returns:\n",
    "        List[Dict[str, Any]]:\n",
    "            - A list of processed objects.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Processing total objects: {len(source_objects)}\")\n",
    "\n",
    "    # Preprocess the raw data to ensure we're working with a clean set\n",
    "    preprocessed_objects = preprocess_raw_data(source_objects)\n",
    "    logger.info(f\"Successfully pre-processed {len(preprocessed_objects)} object(s)\")\n",
    "\n",
    "    processed_objects = []\n",
    "    for obj in preprocessed_objects:  # Iterate over preprocessed raw data\n",
    "        try:\n",
    "            obj_id = obj.get(\"id\")  # Ensure obj_id is extracted\n",
    "            logger.info(f\"Processing object with ID: {obj_id}\")\n",
    "\n",
    "            # Transform source object to standard format\n",
    "            standard_obj = transform_source_object(obj, attribute_mapping)\n",
    "\n",
    "            # Apply classification restrictions\n",
    "            processed_obj = apply_restrictions(standard_obj, restrictions_config)\n",
    "\n",
    "            if processed_obj is not None:\n",
    "                processed_objects.append(processed_obj)\n",
    "        except Exception as e:\n",
    "            logger.error(\n",
    "                f\"Unexpected error processing object {obj.get('id')}: {str(e)}\"\n",
    "            )\n",
    "\n",
    "    # Fix container types to match schema requirements\n",
    "    cleaned_processed_objects = fix_container_types(processed_objects)\n",
    "    logger.info(\"Fixed container types to match schema requirements\")\n",
    "\n",
    "    return cleaned_processed_objects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049593e",
   "metadata": {},
   "source": [
    "### Load into Databricks Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c20014",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add the functions for loading the standard objects into delta table\n",
    "def get_databricks_connection():\n",
    "    \"\"\"Create a connection to Databricks using credentials from environment variables.\"\"\"\n",
    "    try:\n",
    "        logger.info(\"Connection Established...\")\n",
    "        return sql.connect(\n",
    "            server_hostname=os.getenv(\"SERVER_HOSTNAME\"),\n",
    "            http_path=os.getenv(\"HTTP_PATH\"),\n",
    "            access_token=os.getenv(\"ACCESS_TOKEN\"),\n",
    "        )\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to connect to Databricks: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcde6d2",
   "metadata": {},
   "source": [
    "## Main Execution\n",
    "\n",
    "#### Execute the pipeline logic one cell at a time\n",
    "* The code below is taken from [`oms_data_pipeline.py`](../src/pipelines/oms_data_pipeline.py) and executes it line-by-line\n",
    "* This can be used for debugging or simply to see the sequential behavior of the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eee51c7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the paths for data, config, and output\n",
    "data_path = \"../data/1_raw/input\"\n",
    "output_path = \"../data/2_processed/output\"\n",
    "\n",
    "attribute_mapping_path = \"../configs/attribute_mapping.yaml\"\n",
    "valid_attributes_path = \"../configs/valid_attributes_6_19_25.yaml\"\n",
    "restrictions_path = \"../configs/classifications_config.yaml\"\n",
    "attribute_report_path = \"../configs/unexpected_attributes_report.yaml\"\n",
    "# schema_path = \"../configs/schemas/standard_object_schema_v1.4.json\"\n",
    "schema_path = \"../configs/schemas/standard_object_schema_v1.5.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af8f21a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the source objects\n",
    "source_objects = fetch_all_objects(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db167d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load attribute mapping and restrictions from configuration file\n",
    "attribute_mapping = load_attribute_mapping(attribute_mapping_path)\n",
    "valid_attribute_names = load_valid_attribute_names(valid_attributes_path)\n",
    "restrictions_config = load_classification_config(restrictions_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbcd65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect unexpected attribute names across all input objects\n",
    "detect_unexpected_attribute_names(\n",
    "    source_objects, valid_attribute_names, attribute_report_path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d5c25d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare dates in source objects\n",
    "source_objects = prepare_dates(source_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3948057",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform each source object into the standard format\n",
    "standard_objects = process_objects(\n",
    "    source_objects, attribute_mapping, restrictions_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72faba3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the cleanup function to all standard objects\n",
    "cleaned_standard_objects = [clean_object(obj) for obj in standard_objects]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ae5303",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the validations on the standard objects\n",
    "validation_summary = run_validations(cleaned_standard_objects, schema_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a0376f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the cleaned standard objects to JSON files\n",
    "save_standard_objects(output_path, cleaned_standard_objects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9b844c",
   "metadata": {},
   "source": [
    "### Load to Databricks\n",
    "* The final step is to load the data into Databricks delta table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db4ba959",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cds",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
